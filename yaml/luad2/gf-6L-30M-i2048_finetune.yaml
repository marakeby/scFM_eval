run_id: original
dataset:
    module: data.gf_loader
    class: GFLoader
    path: mnt/DATA/luad_brca/luad/luad_analysis2.h5ad #path is relative to DATA_PATH in setup_path.py
    load_raw: True
    label_key: group_id
    batch_key: sample_id
    layer_name: X  #if data exist in another layer (e.g. adta.layers['counts']). The data will be copied to adata.X
    train_test_split: data_splits/luad/luad2/train_test_split.json #path is relative to BASE_PATH in setup_path.py
    cv_splits: data_splits/luad/luad2/cv_splits.json #path is relative to BASE_PATH in setup_path.py

qc:
    min_genes: 10
    min_cells: 10

preprocessing:
    normalize: True
    target_sum: 10000
    apply_log1p: True

hvg:
    batch_key: batch
    n_top_genes: 2048
    flavor: seurat # expect logarithmized data
    
    
embedding:
    skip: True # dont excute this (embedding) step
    viz: False
    eval: False

classification:
    module: models.gf_finetune
    class: GFFineTuneModel
    params:
        model: Geneformer-V1-10M
        model_dir: /home/jupyter/mnt/MODELS/
        dict_dir: /home/jupyter/mnt/gf_dicts_v1 #genefromer 1
        model_input_size: 2048
        batch_size: 64
        epoch: 3
        # freeze_layers: 2
        # learning_rate: 0.00001
        train_funcs: 
            # - avg
            # - mil
            - vote
        cv: True
        label_map:
              treatment_naive: 0
              TKI: 1
    viz: True
    eval: True
